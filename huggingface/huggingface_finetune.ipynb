{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "royal-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "floral-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ethical-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import glue_convert_examples_to_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "alike-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import TFBertForSequenceClassification\n",
    "\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "from transformers import TFTrainer, TFTrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-business",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-sector",
   "metadata": {},
   "source": [
    "### mrpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-domain",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = tfds.load('glue/mrpc', with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data))\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pointed-marathon",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(data[0]))\n",
    "print(type(data[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-cuisine",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "len(data[0]['train']) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crazy-morgan",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[0]['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-burning",
   "metadata": {},
   "source": [
    "### imdb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "organized-punishment",
   "metadata": {},
   "source": [
    "!ls -alh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occasional-omaha",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wget: /data2/wangyh/anaconda3/lib/libuuid.so.1: no version information available (required by wget)\r\n",
      "File ‚Äò./data/aclImdb_v1.tar.gz‚Äô already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!wget -nc http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "large-yesterday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: shell redirect > /dev/null\n",
    "\n",
    "!tar -k -xf ./data/aclImdb_v1.tar.gz -C ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "incorporate-warrant",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_imdb_split(split_dir):\n",
    "    \n",
    "    split_dir = Path(split_dir)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    for label_dir in [\"pos\", \"neg\"]:\n",
    "        for text_file in (split_dir/label_dir).iterdir():\n",
    "           \n",
    "            # print(type(text_file))\n",
    "            # print(text_file)\n",
    "            # print(text_file.read_text())\n",
    "            # raise ValueError(' ')\n",
    "            \n",
    "            texts.append(text_file.read_text())\n",
    "            labels.append(0 if label_dir is \"neg\" else 1)\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "still-miniature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12503\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh ./data/aclImdb/train/pos | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "instructional-sugar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "train_texts, train_labels = read_imdb_split('./data/aclImdb/train')\n",
    "test_texts, test_labels = read_imdb_split('./data/aclImdb/test')\n",
    "\n",
    "print(len(train_texts))\n",
    "print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "twenty-mining",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "<class 'list'>\n",
      "I grew up on Scooby Doo Where Are You, and I still love it. It is one of my favourite cartoons along with Darkwing Duck, Talespin, Peter Pan and the Pirates and Tom and Jerry. This show though is good for kids, the voices are good(Don Messick and Casey Kasem are perfect as Scooby and Shaggy), the theme tune is tolerable and it has some nice animation. However it is rather disappointing. I normally don't mind Scrappy, but when he appears to be like the main character, it gets annoying fast. Complete with the catchphrase Puppy Power, Scrappy is somewhat more annoying than usual. Also half the gang are missing after the first year, somehow it didn't feel like Scooby Doo. And the jokes and the story lines were in general lame and unoriginal, very little chasing monsters or unmasking the baddies. All in all, not as bad as Shaggy and Scooby Doo:Get a Clue, but this show is disappointing. 4/10 for the animation, voices, theme tune and the fact it is nice for kids. Bethany Cox\n"
     ]
    }
   ],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)\n",
    "\n",
    "print(len(train_texts))\n",
    "print(type(train_texts))\n",
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "modern-sleeve",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "photographic-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see huggingface_predict\n",
    "# batch encodings\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baking-polyester",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: maybe for Trainer, TFTrainer only\n",
    "# cause AutoModel use upper encodings directly\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-backup",
   "metadata": {},
   "source": [
    "## Model - Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuous-documentation",
   "metadata": {},
   "source": [
    "### mrpc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "conditional-lambda",
   "metadata": {},
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "different-forge",
   "metadata": {},
   "source": [
    "# tokenize MRPC and convert it to a TensorFlow Dataset object\n",
    "\n",
    "train_dataset = glue_convert_examples_to_features(data[0]['train'], tokenizer, max_length=128, task='mrpc')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fantastic-hello",
   "metadata": {},
   "source": [
    "validation_dataset = glue_convert_examples_to_features(data[0]['validation'], tokenizer, max_length=128, task='mrpc')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "immediate-network",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "electoral-learning",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.batch(3).repeat(2)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "significant-maintenance",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.range(8)\n",
    "dataset = dataset.shuffle(5).batch(3).repeat(2)\n",
    "list(dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "medium-option",
   "metadata": {},
   "source": [
    "# TODO: shuflle, batch, repeat Êåâ‰πãÂâçÁöÑËÆ∞ÂøÜÔºå‰ºº‰πéÊúâÈ°∫Â∫èÂÖ≥Á≥ªÔºü\n",
    "# TODO: epoch=2, Â∞±ÂøÖÈ°ªÊâãÂä®repeat 2Ôºü\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vertical-washington",
   "metadata": {},
   "source": [
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "raw",
   "id": "working-amateur",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "black-civilian",
   "metadata": {},
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "caring-segment",
   "metadata": {},
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dutch-offer",
   "metadata": {},
   "source": [
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "athletic-wells",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.fit(train_dataset, epochs=2, steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trying-watson",
   "metadata": {},
   "source": [
    "## Model - Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-payroll",
   "metadata": {},
   "source": [
    "### mrpc"
   ]
  },
  {
   "cell_type": "raw",
   "id": "gorgeous-wonder",
   "metadata": {},
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(\"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "theoretical-algeria",
   "metadata": {},
   "source": [
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    #warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "casual-reply",
   "metadata": {},
   "source": [
    "print(train_dataset.cardinality().numpy())\n",
    "print(tf.data.INFINITE_CARDINALITY )\n",
    "print(tf.data.UNKNOWN_CARDINALITY)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "limited-natural",
   "metadata": {},
   "source": [
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,    # tensorflow_datasets training dataset\n",
    "    eval_dataset=validation_dataset       # tensorflow_datasets evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "statutory-seeker",
   "metadata": {},
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-distance",
   "metadata": {},
   "source": [
    "### imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "addressed-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# 1. param: warmup_steps\n",
    "# 2. ./results file meaning\n",
    "# 3. ./logs file meaning\n",
    "\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    \n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    \n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    \n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wireless-drain",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_19', 'pre_classifier', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "with training_args.strategy.scope():\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TFTrainer(\n",
    "    model=model,                         # the instantiated ü§ó Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "internal-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4d24c21130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7f4d24c21130>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-74ab856290cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tf2.4/lib/python3.7/site-packages/transformers/trainer_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed_training_steps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_logging\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_iter\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.4/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    616\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m     raise NotImplementedError(\n\u001b[1;32m    620\u001b[0m         \"numpy() is only available when eager execution is enabled.\")\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.4/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     \"\"\"\n\u001b[1;32m   1070\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf2.4/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%time trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-increase",
   "metadata": {},
   "source": [
    "## Save & Load"
   ]
  },
  {
   "cell_type": "raw",
   "id": "incident-saturn",
   "metadata": {},
   "source": [
    "model.save_pretrained('./my_mrpc_model/')\n",
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "conventional-blowing",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "pytorch_model = BertForSequenceClassification.from_pretrained('./my_mrpc_model/', from_tf=True)\n",
    "pytorch_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "altered-digit",
   "metadata": {},
   "source": [
    "## Ref"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-chancellor",
   "metadata": {},
   "source": [
    "[1] https://huggingface.co/transformers/training.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-quilt",
   "metadata": {},
   "source": [
    "dataset shuffle, batch, repeat <br>\n",
    "\n",
    "[2] https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#shuffle <br>\n",
    "[3] https://tensorflow.google.cn/api_docs/python/tf/data/Dataset?hl=en#batch <br>\n",
    "[4] https://tensorflow.google.cn/api_docs/python/tf/data/Dataset#repeat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-brooklyn",
   "metadata": {},
   "source": [
    "dataset cardinality <br>\n",
    "\n",
    "[5] https://tensorflow.google.cn/api_docs/python/tf/data/Dataset?hl=en#cardinality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-allergy",
   "metadata": {},
   "source": [
    "[6] https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "common-negotiation",
   "metadata": {},
   "source": [
    "shell &> <br>\n",
    "\n",
    "[7] https://stackoverflow.com/questions/13341702/how-do-i-turn-off-the-output-from-tar-commands-on-unix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
