{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import jieba\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "# from multiprocessing import get_context\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-b9ee7a6d8670>:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 10**5)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def apply_parallel(df, func, n=-1):\n",
    "    n = os.cpu_count() if -1 == n else n\n",
    "    \n",
    "    df_ = np.array_split(df, n)\n",
    "    with Pool(n) as p:\n",
    "    # with get_context('spawn').Pool(n) as p:\n",
    "        dfr = pd.concat(p.map(func, df_))\n",
    "        # r = p.map(func, df_)\n",
    "    p.join()\n",
    "    \n",
    "    # dfr = pd.Series(chain.from_iterable(r))\n",
    "    # print(df.shape, dfr.shape)\n",
    "    # print(dfr.head(100))\n",
    "    \n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, filename):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path, filename), index_col=0)\n",
    "    # print(df.shape)\n",
    "    # df.head(1)\n",
    "\n",
    "    texts = df[['content']]\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = texts[texts.content == 'é«˜ä»·æ”¶å´å›½é«˜è¿å·ï¼Œæœ‰æ„çš„ç§èŠï¼Œæ™šä¸Šä¹ç‚¹ç»Ÿä¸€å›å¤ï¼']\n",
    "# texts = texts[texts.content == 'æœ‰æ„è¯·åŠ VX18526109947']\n",
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts[texts.str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter_localization(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{localization:[0-9]+\\-[0-9]+}$')].reset_index(drop=True)\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_battle(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{battle:[0-9]+,ã€.*ã€‘.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_system_info(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pd.DataFrame(texts.head(10))\n",
    "texts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lower(texts):\n",
    "\n",
    "    texts['tokens'] = texts['content'].str.strip().str.lower()\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_words(texts):\n",
    "\n",
    "    sw = [r'\\s+', r'{localization:[0-9]+\\-[0-9]+}', 'ä¸¶']\n",
    "\n",
    "    for _ in sw:\n",
    "        texts['tokens'] = texts['tokens'].str.replace(_, '')\n",
    "        # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_only(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].str.isnumeric()]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chars(texts):\n",
    "\n",
    "    dict_merge_chars = {'è´æˆ‹' : 'è´±', 'çŠ­å¥' : 'ç‹—', 'å¥³é©¬' : 'å¦ˆ'}\n",
    "\n",
    "    for k, v in dict_merge_chars.items():\n",
    "        texts['tokens'] = texts['tokens'].str.replace(k, v)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def tokenize(df):\n",
    "    return df.apply(jieba.lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenization(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(jieba.lcut)\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], jieba.lcut)\n",
    "    # print(len(texts))\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], tokenize)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chars + numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_split_cn(df):\n",
    "    return df.apply(split_chars_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cn(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(split_chars_numbers)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_split_cn)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_split = {\n",
    "'å¤§é‡å‡º' : ['å¤§é‡', 'å‡º'],\n",
    "'åŠ å¾®ä¿¡' : ['åŠ ', 'å¾®ä¿¡'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_special_words(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda x: list(chain.from_iterable([dict_split.get(i,[i]) for i in x])))\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge2 = [['èµ„æº', 'å•†'], ['å¼ ', 'è¾½'], ['c', 'ä½'], ['éƒ¡', 'åŸ'], ['æ´—æ´—', 'ç¡'], ['å®ˆ', 'ä¸ä½'], ['åˆš', 'æœ‰äº‹'], ['å¼ƒ', 'å‘'], ['æ–°æ‰‹', 'æœ'], ['å¸¦', 'å…µ'], ['è€', 'æ ·å­'], \n",
    "               ['å†’ä¸ª', 'æ³¡'], ['ç™½', 'å«–'], ['äº²', 'å¯†åº¦'], ['æ‰“å¯‡', 'åŒª'], ['å‘¨å¹´', 'åº†'], ['å¸¦', 'èŠ‚å¥'], ['ç©', 'æ¸¸æˆ'], ['æ–°', 'ç‰ˆæœ¬'], ['ç§¯ç‚¹', 'å¾·'], ['ç‚¸', 'çŸ¿'], ['èµ›å­£', 'æœ'], \n",
    "               ['é‚º', 'åŸ'], ['æƒ³', 'åŠæ³•'], ['ä¹Œ', 'éª“'], ['é¾Ÿ', 'å­™'], ['çº¢', 'æ‰‹æŒ‡'], ['æ­»', 'å¦ˆ'], ['2', 'é˜Ÿ'], ['ç­‰', 'ä¼š'], ['æ‰“', 'ä¸è¿‡'], ['å¥½', 'å§'], ['ç­‰', 'ä¸‹'], ['æ‰“', 'æ‰'], \n",
    "               ['å»', 'å§'], ['ç­‰', 'ä¸€ä¸‹'], ['æ‰“', 'ä¸€ä¸‹'], ['æ‰“', 'å“ª'], ['æ”¶å…µ', 'çº¿'], ['æ’¤', 'å§'], ['ä¸', 'å®¢æ°”'], ['ç¨', 'ç­‰'], ['ä¸', 'æ‡‚'], ['æ‰“', 'ä¸äº†'], ['ä¸ä¼š', 'å§'],\n",
    "              ['ä¸', 'ç¨³å®š'], ['ä¸‹', 'ä¸€ä¸ª'], ['äºº', 'ä¸å¤Ÿ'], ['åŠªåŠ›', 'ä¸­'], ['èµŒ', 'ä¸€æŠŠ'], ['åˆ°', 'ç‚¹'], ['1', 'é˜Ÿ'], ['æ²¡çœ‹', 'æ‡‚'], ['+', '1'], ['åˆä¸ª', 'å½±'], ['ç‡•å­', 'å'],\n",
    "              ['ä¸€', 'æ¢­å­']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge3 = [['é’å±±', 'ä¸æ”¹', 'ç»¿æ°´é•¿æµ'], ['æ‰“', 'ä¸', 'æ‰“'], ['æ¥', 'ä¸æ¥']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge3(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(1<i):\n",
    "        if [x[i-2],x[i-1],x[i]] in list_merge3:\n",
    "            r.insert(0, ''.join([x[i-2],x[i-1],x[i]]))\n",
    "            i -= 3\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "\n",
    "    if 1 == i:\n",
    "        r.insert(0, x[1])\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge2(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(0<i):\n",
    "        if [x[i-1],x[i]] in list_merge2:\n",
    "            r.insert(0, ''.join([x[i-1],x[i]]))\n",
    "            i -= 2\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "            \n",
    "    if 0 == i:\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_3(df):\n",
    "    return df.apply(merge3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_words_3(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge3)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_3)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_2(df):\n",
    "    return df.apply(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_words_2(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge2)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_2)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_stopwords(path):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            stopwords.extend([w.strip() for w in f.readlines()])            \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords = get_stopwords('/home/wangyh/project/document_cluster/dicts/')\n",
    "print(len(stopwords))\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ads detect only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['â€¦', 'âŠ™', 'âˆ€', 'à²¡', 'Ï‰', 'à²¡', 'ğŸ˜‚', 'ğŸ˜œ', 'è°¢è°¢', 'å“ˆ', 'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'æ˜å¤©', 'æ™šä¸Š', 'ä¸­åˆ', 'åˆ·', 'åˆ·åˆ·', 'éœ', 'å¼€', 'å¼€å¼€', 'å‡ ç‚¹', 'æ²¡', 'æ”¹', 'å†›å›¢',\n",
    "     'æ•£', 'ç»“æŸ', 'åˆ‡ç£‹', 'é˜Ÿä¼', 'æ—©', 'è¯´', 'æŒ‚å…', 'ä¸¢äºº', 'æ‰“åŒª', 'é©»é˜²', 'æ„Ÿè°¢', 'å†è§', 'æ¥åˆ·', 'æŠ±æ­‰', 'è¡Œå†›', 'é£', 'ç‰›', 'ç‰›ç‰›', 'åšäºº', 'å–„è‰¯', 'èµ', 'èµèµ', 'èµèµèµ',\n",
    "     'å…„å¼Ÿ', 'é£äº‘', 'æ–‡æ˜', 'é£è¿‡æ¥', 'æ‹', 'å®Œ', 'æ‹‰', 'æ±‚è™', 'æ‰¾', 'æ¸£', 'å–Š', 'æ‹ç…§', 'è¯•è¯•', 'è°¢', 'æŠ¢', 'å‘', 'æ²¡äº‹', 'å–Š', 'å ', 'åš', 'è¯•', 'ä»Šæ™š', 'æ›´æ–°', 'ç¡è§‰', \n",
    "      'èµ°', 'å¸®å¿™', 'å¸®', 'ä¸€ä¼š', 'åƒ', 'â•¯', 'â•°', 'æºœ', 'å·', 'åˆ·ä¸åˆ·', 'â†', 'à²¥', 'åŠ é€Ÿ', 'æ¥', 'åš', 'è¿™æ˜¯', 'å‘±å”§', 'å…ˆæ‰“', 'å¿åŸ', 'å¿', 'æ‰“ä¸åŠ¨', 'ä¸ç”¨', 'å–é…’', 'ä¸å¥½æ„æ€',\n",
    "     'å”§å”§', 'æ²¡äº‹', 'é€Ÿåº¦', 'å…ˆ', 'æ´¾', 'å¼„', 'ç•™ä¸ª', 'çºªå¿µ', 'à¸‡', 'â€¢', 'Ì€', 'â€¢', 'Ì', 'æŠ¥å', 'æ–°', 'æ–°ç‰ˆæœ¬', 'è½®å­', 'å—²', 'å¥³é©¬', 'ä¸€æ¡', 'èˆ”', 'èŠèŠ±', 'å±è‚¡', 'ç§ƒé¡¶', 'åä»£', 'å…”å­', \n",
    "     'é¹°é…±', 'å…¨å®¶', 'å‘è´§', 'åˆç…§', 'æ¸¸æˆ', 'å¤ª', 'å›¢', 'é‡Œ', 'è¯…å’’', 'åˆå½±', 'ä¸€é˜Ÿ', 'äºŒé˜Ÿ', 'äºŒå›¢', 'ä¸‰å›¢', 'ä¸€ç¾¤ç¾¤', 'é˜¿çŒ«', 'éª—', 'åˆ ', 'ç»Ÿä¸€', 'ç­¾åˆ°', 'è‡ªåŠ¨', 'è€å­', 'è¯´è¯', \n",
    "      'çªå›ŠåºŸ', 'æ­»å¦ˆ', 'æ²¡å¦ˆ', 'çŸ­å‘½', 'æ¸£ç§', 'æˆå¤©', 'åªä¼š', 'äº²éº»', 'æ‰“æ³¡', 'äº²è·Œ', 'æ­»', 'å ', 'éª‚', 'èµ¢', 'æ±Ÿæ¹–', 'æ•…äºº', 'ç›¸é€¢', 'é’å±±', 'ä¸æ”¹', 'ç»¿æ°´é•¿æµ', 'ä¸‡äº‹', 'é¡ºé‚',\n",
    "     'å°”éš†', 'å’šé”µ', 'è¿›', 'å·®', 'è¿œ', 'åˆ', 'ï¼Œ', 'éƒ½', 'æ˜¯', 'å°±æ˜¯', 'çš„', 'æ›´å¥½', 'å‘å±•', 'åˆ©ç”¨', 'ï¼Œ', 'å¤§å®¶', 'ä¸€èµ·', 'ä½œæˆ˜', 'æœ‰', 'å’Œ', 'ä½ ', 'æˆ‘', 'è¿˜', 'åœ¨çº¿', 'ã€‚', '*', 'ã€', ':',\n",
    "     'æ‚¨', 'èƒ½', 'ä¸º', 'äº†', 'å°±', 'ï¼ˆ', 'ï¼Ÿ', 'å—', 'æ¥', 'äº†', 'èƒ½', 'ä¸èƒ½', 'å¤§', 'å˜›', 'è¢«', 'é‚£', 'åœ¨', 'ï¼', 'ä»–', 'å†', 'è‡ªå·±', 'åæ ‡', 'æˆªå›¾', 'ï¼', 'å‡†å¤‡', 'é€€åœº', '2é˜Ÿ', 'è¿™', \n",
    "     'ç§¯ç‚¹å¾·', 'ä¹Ÿ', 'è®©', 'ä»–ä»¬', 'å•Š', 'ç°åœ¨', 'ä¸ºäº†', 'ä½ å¥½', 'æ‚¨å¥½', 'æœ¬', 'æœ‰å¿—', 'é’å¹´', 'å…±åŒ', 'è¿æ¥', 'ç›´æ¥', 'ç”³è¯·', 'èœ€å›½', 'åŠ æ²¹', 'ç ¸ç§', 'ç‹—', 'æ±—é—´', 'é­ä¸»', \n",
    "      'ç¥–å®—', 'ç°å®', 'åœ°ä½', 'ä¹è¶£', 'å‘¢', 'è®¤è¯†', 'æ‹¿', 'ä¸ª', 'æ¯”', 'å½“', 'ä»¬', 'ä¸€åœº', 'æŒä¹…æˆ˜', 'ç›¸ä¿¡', 'èƒœåˆ©', 'ç»ˆå°†', 'å±äº', 'æˆ‘ä»¬', 'å›¢ç»“', 'ä¸€å®š', 'èƒœåˆ©', 'æ¬¢è¿', 'æ‰€æœ‰', \n",
    "      'å´å›½', 'å¤§è€', 'é»‘', 'åˆç§°', 'çœŸäºº', 'å¹¿å¤§', 'ç¾¤ä¼—', 'è¿œç¦»', 'äº²ç ', 'å—·å—·å«', 'å­é¡º', 'å„¿', 'æ‡†', 'å¾—', 'è¿™ä¸ª', 'å–”', 'çŒª', 'æ‹±', 'æµ‘èº«', 'æºƒçƒ‚', 'æ­£åœ¨', 'å–', 'è„“è¡€', 'ç–—ä¼¤',\n",
    "     'å¯æ€œ', 'å®¶é‡Œ', 'å¾—', 'å‰©', 'çŸ¥é“', 'ä¸', 'å®¹æ˜“', 'çˆ¶æ¯', 'ç­‰', 'ç€', 'æ‹¿', 'é’±', 'ä¹°', 'æ£ºæ', 'ä¸‹è‘¬', 'å‘¢', 'å¯æ˜¯', 'é‚£ç‚¹', 'çˆ¶æ¯', 'çƒ§çº¸', 'ä¸å¤Ÿ',\n",
    "      'ä¸', 'çŸ¥é“', 'å›', 'äº‘å¤©', 'é™ª', 'æ‰€è°“', 'â€œ', 'â€', 'è¿˜æ˜¯', 'åƒ', 'ç™çš®ç‹—', 'ä¸€æ ·', 'ç»§ç»­', 'ç•™åœ¨', 'å¤§è…¿', 'å¯ä»¥', 'æŠ±', 'å›¢é‡Œ', 'å½“å¥´æ‰', 'æ‹­ç›®ä»¥å¾…',\n",
    "     'ç§åœ°', 'ç°å®', 'æ®‹é…·', 'å†œæ°‘', 'ä»¬', 'è¾›è‹¦', 'å•¦', 'ä¸è¾è¾›è‹¦', 'åšæŒ', 'å¸½å­', 'å°', 'å†œæ°‘', 'æ€ç»´', 'çœŸæ˜¯', 'å•çº¯', 'å¯çˆ±', 'æƒ¹', 'äºº', 'ç–¼', 'å‘¢', 'ä¸', 'çŸ¥é“', 'ä¸', 'æ‰“', \n",
    "      'å®æœ', 'å‚»', 'ç¬”ç‹', 'äºº',  'å‘¢', 'æ‰“', 'å§', 'æ­¤', 'ä¸è‚–', 'å­', 'è¿˜æœ‰', 'äºº', 'é›†ç»“', 'ç•™å¿µ', 'æœ€å', 'ä¸€æœŸ', 'å¥½', 'å‡‘ä¸ª', 'è„¸ç†Ÿ', 'ä»€ä¹ˆ', 'æ„æ€', 'å—¯', 'å¥½', 'æƒ…å†µ', 'äºŒè´§',\n",
    "     'ä»Šå¤©', 'æ•Œäºº', 'æœ‹å‹', 'è¿™é‡Œ', 'æ‰“', 'å¯èƒ½', 'å»¶è¿Ÿ', 'å‡ åˆ†é’Ÿ', 'å°½é‡', 'ä¿è¯', 'æ¯', 'ä¸€ä½', 'è¿™ä¸ª', 'æ—¶å€™', 'å‘µå‘µ', 'å‘µ', 'å¥½', 'å˜', 'æ©æ€¨', 'ä¸€ç¬‘', 'æ³¯', 'æ©ä»‡',\n",
    "     'å•¥', 'æƒ…å†µ', 'ä½ ä»¬', 'å“¦', 'å¥½', 'é¡¶æˆ˜', 'æ¸…äºº', '(', ')', 'åˆš', 'ä¸Šçº¿', 'æˆ‘ä»¬', 'åŠ æ²¹', 'æˆå‘˜', 'å…¨ä½“', 'é›†åˆ', 'æ°‘å¿ƒ', 'ä»»åŠ¡', 'å§', 'å“ª', 'è®¤è¯†', 'æ¸…æ¥š', 'ä¹ˆ', 'åˆ†é’Ÿ', 'æ‰“ä¸è¿‡', \n",
    "     'åˆ«', 'å‘€', 'æˆ˜åŠ›', 'ä»¥å', 'å†å‡', 'ç•™ç‚¹', 'å…µ', 'å»', 'æœ‰äºº', 'åœŸåŒª', 'æ¥ä¸ª', 'å¸®å¸®å¿™', 'ï¼š', 'ï¼‰', 'æœ‰äºº', 'ç¢°ç“·', 'å‰å®³', 'å‘€', 'æ€ä¹ˆ', 'ä¸ç†', 'å› ä¸º', 'è§‰å¾—', 'å’‹æ ·',\n",
    "      'æ…¢æ…¢', 'ç©å§', 'äº', 'å¤§å‘', 'åˆ«', 'äº’ç›¸', 'ä¼¤å®³', 'å‘Šè¯‰', 'æ‰“é‡', 'æœ‰ç‚¹', 'è¿‡åˆ†', 'å›å®¶', 'å…¬å‘Š', 'çœ‹', 'å»', 'å°†', 'æˆ‘å¼€', 'ç›¸å¿˜', 'äº', 'åˆ°', 'æ—è¾¹', 'ä¹', 'æœ‰ç¼˜åƒé‡Œ', 'ç›¸ä¼š',\n",
    "     'æœ‰ç¼˜', 'å„ä½', 'çœŸ', 'æ— èŠ', 'åˆ†é’Ÿ', 'æˆ‘æ¥', 'çœ‹çœ‹', 'æ', 'æ²¡æœ‰', 'ä¸€ä¸‹', 'ä¸€ä¸ª', 'ä¸Š', 'ä¸‹', 'ç”¨', 'ä¸è¦', 'ä½ç½®', 'è¿™ä¹ˆ', 'ç¹è£', 'è°', 'ç©', 'ç§’', 'æŠŠ', 'å¤šå°‘', 'å¤š', 'é­å›½',\n",
    "      'ä¸€äºŒä¸‰å››äº”', 'è¿‡æ¥', 'ä¹ç‚¹', 'é•¿å®‰', 'æ¬¡', 'ä¸æ˜¯', 'å¼€å§‹', 'é˜µå®¹', 'ä»¥ä¸Š', 'æ—¶é—´', 'å¾ˆ', 'æ¸…', 'å¸¦', 'åŸ', 'æ‰“é£', 'ä¼š', 'å’¯', 'åœ°ç‚¹', 'éƒ¡åŸ', 'ç•™å½±', 'æ´›é˜³', 'æƒ³', 'æœ‰æ²¡æœ‰', \n",
    "      'æ', 'ç¥', 'æå‰', 'éšä¾¿', 'çœ‹è§', 'ä¸ªäºº', 'èµ·æ¥', 'ä¸€ç‚®', 'ä¸è¡Œ', 'å«', 'å¯¹', 'è½ä½', 'ä¸Šè½¦', 'å¤©å¤©', 'ä¸å¥½', 'ä»¤', 'åˆ†', 'åŒº', 'é«˜', 'ç‹', 'å…¶ä»–', 'é€šå‘Š', 'å›´åŸ', 'è”ç›Ÿ',\n",
    "     'å¿«', 'å…µçº¿', 'åœ°æ–¹', 'å¤Ÿ', 'å’‹', 'çº¿', 'å°æ—¶', 'è·‘', 'åé¢', 'æ— ', 'èµ·', 'é‚£ä¸ª', 'èœ€', 'æ•£äºº', 'éƒ¡', 'æ‰€æœ‰äºº', 'æœ¬äºº', 'åŸå§‹', 'å¨±ä¹', 'èµ¶ç´§', 'ä¸€ç‚¹', 'cä½', 'é”™', 'å»ºç­‘',\n",
    "     'æˆéƒ½', 'ä¸€ç¯', 'å‡‘', 'å¸Œæœ›', 'æ’¤', 'é£è¿‡å»', 'é…åˆ', 'å…¶ä»–äºº', 'è¿‡', 'å‡ºå…µ', 'é‚£ä¹ˆ', 'éšæœº', 'æœ€å¥½', 'å…¨éƒ¨', 'æ‰', 'ä¸€ç›´', 'é¢†', 'ä¸­', ',', 'å¯', 'ç°', 'å¼€å¿ƒ', 'è·Ÿ', 'ä¸»å…¬',\n",
    "     'ç‚¸', 'é©¬ä¸Š', 'æœ¬æœ', 'ç…§', 'è¿‡èŠ‚', 'å¤§å°', 'æŠ¥ä»‡', 'ä¸‰å›½', 'ä»Šå', 'ä»¥å‰', 'æ¶ˆ', 'å¼Ÿå…„ä»¬', 'æ—©åˆ°', 'å·²ç»', 'è‡ªç«‹', 'åŸæ± ', 'å‡†æ—¶', 'ç§ç”°', 'æ˜¥èŠ‚', 'å·²æ”¾', 'ä¸‹ä½', 'å¤„',\n",
    "     'æ–¹å‘', 'å‚åŠ ', 'è¯¥', 'æµªè´¹', 'ç…§ç‰‡', 'ä¸–ç•Œ', 'è§', 'ä¸€æ¬¡', 'é›†ä½“', 'åšä¸ª', 'åˆ°ä½', 'å·®è·', 'å¤©', 'å°±ä½', 'æ‰“åŸ', 'æ²¡å‡ºæ¯', 'å¥‡æ€ª', 'å¥½åƒ', 'åŸå¤´', 'ä¸äº†', 'å“ªä¸ª', 'ä¸åˆ°', \n",
    "     'æ²¡å…µ', 'å½»åº•', 'æŠ¥', 'è®¸æ˜Œ', 'å“¥å“¥', 'åˆšåˆš', 'å†æ¥', 'åˆšæ‰', 'å›æ¥', 'å¦‚æœ', 'å‡ºæ¥', 'å‡ æ¬¡', 'æ€»æ˜¯', 'åº”è¯¥', 'å¥½å¤š', 'è¿‘', 'é•¿æ²™', 'ä¸€å…µ', 'ä¸æ¥', 'å¤ªè™š', 'éšæ„', 'åªæœ‰',\n",
    "     'ç­‰ä¼š', 'æ¡æ¸£ç§', 'æ­¤æ—¶æ­¤åˆ»', 'è¢«æ§', 'ç¬¬ä¸€', 'æ·±æ›´åŠå¤œ', 'å¹³æ°‘', 'éœ¸æœ', 'è„¸', 'æœ¬äº‹', 'æ‰“å‡º', 'åˆ«è¯´', 'æ§å·', 'æœ‰é’±', 'åˆ«äºº', 'è¢«æ§', 'æ®‹', 'ç‹—ä¸œè¥¿', \n",
    "      'åœ°å„¿', 'éš†å†¬', 'å¼º', 'å’šå’š', 'é”µ', 'æ•´ä¸ª', 'æŒ¨ä¸ª', 'ç¾¤', 'bb', 'ç¬”', 'å†…', 'é—´', 'å†…', 'ç•œç”Ÿ', 'å­—', 'å°å', 'ç«¥å­', 'å–œ', 'ç”·äºº', 'ç”·é£', 'è€…', 'ä¸æ€•', 'py',\n",
    "     'ç‹—å¥´æ‰', 'å˜´ä¸Š', 'ç«‹äºº', 'è®¾æ—¶', 'è¿', 'å½“çœŸ', 'ä½†', 'ç»ä¸èµ·', 'ä»»ä½•', 'è€ƒéªŒ', 'å•ªå•ª', 'æ‰“è„¸', 'ä¸¤é¢ä¸‰åˆ€', 'å°äºº', 'æ™š', 'ç‚¹', 'è‡ªç”±', 'å¼€æ”¾', '1é˜Ÿ', 'å¹³å®‰', 'å–œä¹', 'æœˆ',\n",
    "     'è§£æ•£', 'éº»çƒ¦', 'éšè¿', 'ä¸œç€›', 'é¬¼å­', 'é‡ç§', 'å…¨é£', 'ä¿æŒ', 'é˜Ÿå½¢', 'å¯’æ±Ÿ', 'å­¤å½±', 'ä½•å¿…', 'æ›¾', 'ç›¸è¯†', 'ç¡', 'æ™šå®‰', 'æ‰‹æœº', 'æ²¡ç”µ', '123', 'èŒ„å­', 'æ”¶å…µçº¿', 'å‡', \n",
    "      'è¿›æ­¥', 'å¾ˆå¤§', 'æ»¡çº§', 'é‡', 'å†ä¹Ÿ', 'ä¸æƒ³', 'æ¬ºè´Ÿ', 'å¼±å›¢', 'æ¬ºè´Ÿ', 'æœ¬å›½', 'å¼±å°', 'åˆ†æ‰‹', 'æ‰“æ‰', 'è¡Œ', 'æ•£ä¼™', 'è‡­è¡¨å­', 'ä¸€ç»„', 'è¿‡å»', 'æ‰', 'å…¨ä¸€ç¯', 'æ ¼å­',\n",
    "     'æ ¡åœº', 'å‰¯', 'å†ä¹Ÿ', 'ä¸è§', 'å“', 'è·³', 'å‚»ç¬”', 'æ¯å¤©', 'æ—©ç‚¹', 'ä¼‘æ¯', 'ç§°å·', 'åºŸ', 'å…«å§“', 'å®¶å¥´', 'å‡ æŠŠ', 'æ¶æ˜¯', 'å‡ æŠŠ',\n",
    "     'èµ›å­£', 'æœå‰', 'ä¸€å¤©', 'ä¸€è·¯', 'æ¶ªé™µ', 'æœ‰ç©º', 'æŒ‰', 'é¡ºåº', 'æ•´ä½“', 'æ’åˆ—', 'ç‚¹ä¼š', 'å®‰æ’', 'å¤§æ®¿', 'ä¸å¸¦', 'åé˜²', 'å¥½å‹', 'å¾’å¼Ÿ', 'å°è½¦', 'æ­»å…‰å…‰',\n",
    "     'æ˜æ™š', 'é•¿å®‰åŸ', 'æœ¬åŒº', 'æä¾›', 'å…·ä½“åœ°å€', 'æ—¶å‘', 'åŸåˆ°', 'é“è¡€', 'å…·ä½“', 'åŠæ‰“', 'å½•å±', 'æ³¨æ„', 'æœ‰åˆ·', 'æš‚æ—¶', 'ç¦»å›¢', 'å—é—¨', 'å†›æ——', 'è´±ç‹—', 'çˆ½', 'ç‹—å¨˜å…»',\n",
    "    'è¥¿å—', 'é—¨', 'å†›æ——', 'å›´', 'ä¸‰ç¯', 'æŠ“ç´§', 'éª‘å…µ', 'è¿›æ”»',  'æŠ¢æ–°', 'ç‰ˆæœ¬', 'é¦–ç«™', 'å¤©ç‹', 'ç›–åœ°', 'è™', 'å¤§ç™½', 'å–', 'è“è‰²', 'èœ€æ±‰', 'æ——', 'å¤§ç™½', 'å‘Š', 'å¦»',\n",
    "     'é“¶æ²³', 'å¥¥ç‰¹æ›¼', 'æ²™æ¯”', 'æ€»å–Š', 'ç›', 'æ²¡é’±åˆ«', 'æ¯”æ¯”', 'èƒ½è€', 'çº³', 'é—·', 'ä¸å¤š', 'é±¼å', 'å„è·¯', 'è‹±é›„', 'å›è´¼', 'æ‰“å®¶åŠ«èˆ', 'åŸ', 'åŒºä¸åˆ†', 'å›½ç±', 'æ‹ä¸ª',\n",
    "     'é’±å¤§ç‹—', 'æœŸå¾…', 'ä¸', 'å†æ¬¡', 'ç›¸é‡', 'æœŸ', 'æˆªæ­¢', 'æƒ³æ¥', 'é£è¡Œ', 'ç©è€', 'ä¸‡ä¸€', 'ä½æˆ˜å›¾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = list(set(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "829"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.extend(sw)\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def filter_sw(df):\n",
    "    \n",
    "    # return pd.Series([w for w in x if w not in stopwords])\n",
    "    \n",
    "    # for w in x:\n",
    "       # print(w, w not in stopwords)\n",
    "    # return [w for w in x if w not in stopwords]\n",
    "    \n",
    "    return df.apply(lambda l: [w for w in l if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda l: [w for w in l if w not in stopwords])\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], lambda l: [w for w in l if w not in stopwords])\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], filter_sw)\n",
    "    # texts = texts.dropna()\n",
    "    \n",
    "    \n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    # print(x)\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    if 2 == len(x): \n",
    "        return [x[0]] if x[0] == x[1] else x\n",
    "    \n",
    "    r = [x[i] for i in range(len(x)-2) if x[i] != x[i+1]]\n",
    "    if x[-1] == x[-2]:\n",
    "        r.append(x[-2])\n",
    "    else:\n",
    "        r.extend(x[-2:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_repeats(texts):\n",
    "\n",
    "    # è°¢è°¢, è°¢è°¢\n",
    "    # å–é…’, å–é…’\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(remove_repeats)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'v' + num\n",
    "\n",
    "def is_v_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and 'v' == x[0] and x[1].isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_v_num(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_v_num)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num + 'çº§'\n",
    "\n",
    "def is_num_ji(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and x[0].isnumeric() and 'çº§' == x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_ji(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_num_ji)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(texts):\n",
    "    # texts['length'] = texts['tokens'].apply(len)\n",
    "    \n",
    "    texts = texts[texts['tokens'].apply(len) > 1]\n",
    "    # print(len(texts))\n",
    "    # print(texts.tail(10))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# df = pd.DataFrame(texts)\n",
    "# df.head(1)\n",
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(texts):\n",
    "\n",
    "    # df = df.astype(str).value_counts().reset_index().rename(columns={0:'cnt'})\n",
    "    # df.head(1)\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].astype(str)\n",
    "    df = texts.groupby('tokens').agg({'tokens':['count'], 'content':[lambda x: pd.DataFrame.head(x,1)]}).reset_index()\n",
    "\n",
    "    df.columns = ['tokens', 'cnt', 'content']\n",
    "    df = df[['tokens','content', 'cnt']]\n",
    "    df = df.sort_values('cnt', ascending=False).reset_index(drop=True)\n",
    "    # df.head(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freq more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_more(df, n):\n",
    "\n",
    "    dfn = df[df['cnt'] >= n]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_between(df, n1, n2):\n",
    "\n",
    "    dfn = df[(df['cnt'] >= n1) & (df['cnt'] < n2)]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df5 = df[df['cnt'] >= 5]\n",
    "print(df5.shape)\n",
    "df5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df4 = df[(df['cnt'] < 5) & (df['cnt'] >= 4)]\n",
    "print(df4.shape)\n",
    "df4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df3 = df[(df['cnt'] < 4) & (df['cnt'] >= 3)]\n",
    "print(df3.shape)\n",
    "df3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for tokens in df3.tokens:\n",
    "    for t in set(eval(tokens)):\n",
    "        counter[t] += 1\n",
    "        \n",
    "counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df[(df['cnt'] < 3) & (df['cnt'] >= 2)]\n",
    "print(df2.shape)\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfn = df[df['cnt'] > 1]\n",
    "print(dfn.shape)\n",
    "# dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df1 = df[df['cnt'] <= 1]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2æœŸ æ ‡æ³¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracts(path, filename):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        texts = load(path, filename)\n",
    "        # texts = texts.head(10)\n",
    "\n",
    "        pipeline = [\n",
    "            filter_localization, \n",
    "            filter_battle,\n",
    "            filter_system_info,\n",
    "\n",
    "            clean_lower,\n",
    "            clean_special_words,\n",
    "            clean_numeric_only,\n",
    "            merge_chars,\n",
    "\n",
    "            tokenization,\n",
    "            split_cn,\n",
    "            split_special_words,\n",
    "            merge_words_3,\n",
    "            merge_words_2,\n",
    "            filter_stopwords,\n",
    "            filter_repeats,\n",
    "\n",
    "            filter_v_num,\n",
    "            filter_num_ji,\n",
    "            filter_length,\n",
    "                   ]\n",
    "\n",
    "        for f in pipeline:\n",
    "            # print(f)\n",
    "            t0 = time.time()\n",
    "            texts = f(texts)\n",
    "            # print(time.time()-t0)\n",
    "\n",
    "        df = extract(texts)\n",
    "\n",
    "        # dfn = freq_more(df, 5)\n",
    "        dfn = freq_between(df, 3, 5)\n",
    "        print(dfn.shape)\n",
    "\n",
    "        return dfn\n",
    "    \n",
    "    except:\n",
    "        print(filename)\n",
    "        traceback.print_exc()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/wangyh/data/yk-sgz2017-chat/data-2-20201223'\n",
    "# filename = '2020_09_25.csv'\n",
    "# filename = '2020_09_24.csv'\n",
    "# filename = '2020_08_20.csv'\n",
    "\n",
    "path = '/home/wangyh/data/yk-sgz2017-chat/data-7-20210120'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time extracts(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.203 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.201 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.200 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.194 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.208 seconds.\n",
      "Loading model cost 1.215 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.201 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "Loading model cost 1.240 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13, 3)\n",
      "(10, 3)\n",
      "(19, 3)\n",
      "(25, 3)\n",
      "(29, 3)\n",
      "(20, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(22, 3)\n",
      "(12, 3)\n",
      "(9, 3)\n",
      "(16, 3)\n",
      "(25, 3)\n",
      "(26, 3)\n",
      "(16, 3)\n",
      "(8, 3)\n",
      "(20, 3)\n",
      "(9, 3)\n",
      "(22, 3)\n",
      "(69, 3)\n",
      "(7, 3)\n",
      "(31, 3)\n",
      "(50, 3)\n",
      "(8, 3)\n",
      "(27, 3)\n",
      "(35, 3)\n",
      "(23, 3)\n",
      "(61, 3)\n",
      "(5, 3)\n",
      "(64, 3)\n",
      "(39, 3)\n",
      "(28, 3)\n",
      "(21, 3)\n",
      "(14, 3)\n",
      "(29, 3)\n",
      "(24, 3)\n",
      "(8, 3)\n",
      "(23, 3)\n",
      "(40, 3)\n",
      "(13, 3)\n",
      "(17, 3)\n",
      "(11, 3)\n",
      "(11, 3)\n",
      "(26, 3)\n",
      "(40, 3)\n",
      "(14, 3)\n",
      "(9, 3)\n",
      "(21, 3)\n",
      "(32, 3)\n",
      "(40, 3)\n",
      "(19, 3)\n",
      "(6, 3)\n",
      "(24, 3)\n",
      "(18, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(16, 3)\n",
      "(24, 3)\n",
      "(18, 3)\n",
      "(11, 3)\n",
      "(12, 3)\n",
      "(20, 3)\n",
      "(24, 3)\n",
      "(10, 3)\n",
      "(13, 3)\n",
      "(10, 3)\n",
      "(19, 3)\n",
      "(18, 3)\n",
      "(19, 3)\n",
      "(19, 3)\n",
      "(31, 3)\n",
      "(16, 3)\n",
      "(13, 3)\n",
      "(7, 3)\n",
      "(34, 3)\n",
      "(20, 3)\n",
      "(27, 3)\n",
      "(8, 3)\n",
      "(13, 3)\n",
      "(8, 3)\n",
      "(23, 3)\n",
      "(21, 3)\n",
      "(17, 3)\n",
      "(17, 3)\n",
      "(9, 3)\n",
      "(19, 3)\n",
      "(19, 3)\n",
      "(26, 3)\n",
      "(14, 3)\n",
      "(30, 3)\n",
      "(15, 3)\n",
      "(21, 3)\n",
      "(7, 3)\n",
      "(18, 3)\n",
      "(25, 3)\n",
      "(20, 3)\n",
      "(6, 3)\n",
      "(14, 3)\n",
      "(4, 3)\n",
      "(22, 3)\n",
      "(32, 3)\n",
      "(21, 3)\n",
      "(20, 3)\n",
      "(25, 3)\n",
      "(5, 3)\n",
      "(37, 3)\n",
      "(10, 3)\n",
      "109.33714628219604\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(path)))\n",
    "t0 = time.time()\n",
    "# dfs = [extracts(path, filename) for filename in tqdm(os.listdir(path))]\n",
    "\n",
    "with Pool(8) as p:\n",
    "    dfs = p.map(partial(extracts, path), os.listdir(path))\n",
    "    # filenames = os.listdir(path)\n",
    "    # for _ in tqdm(p.map(partial(extracts, path), filenames), total=len(filenames)): pass\n",
    "p.join()\n",
    "\n",
    "\n",
    "print(time.time() - t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat(dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1453, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['ç¬¬', '12', 'ç« ', 'ç¬¬', '5', 'å…³', 'å‰¯æœ¬', 'n', 'èƒŒå¼º', 'é€€', 'n', 'é¬¼']</td>\n",
       "      <td>ç¬¬12ç« ç¬¬5å…³å‰¯æœ¬æ‰“Næ¬¡èƒŒå¼ºé€€Næ¬¡ï¼Œæä»€ä¹ˆé¬¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['åŠ ', 'å¾®ä¿¡', 'è¿›ç¾¤']</td>\n",
       "      <td>åŠ æˆ‘å¾®ä¿¡æ‹‰ä½ è¿›ç¾¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['æŒ‚', 'å…æˆ˜']</td>\n",
       "      <td>æŒ‚å…æˆ˜äº†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['çš„è¯', 'åŠ ', 'å¾®ä¿¡', 'è¿›ç¾¤']</td>\n",
       "      <td>ä½ å»çš„è¯åŠ æˆ‘å¾®ä¿¡ï¼Œæˆ‘æ‹‰ä½ è¿›ç¾¤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['é¦–æˆ˜', 'å¥–', '10', 'å¾®ä¿¡', 'ä¸‹ä¸€åŸ', 'ç•™å…µ', 'ç•™', 'å…ƒå®', 'æŠ•çŸ³æœº', 'èµ„æº', '30', 'æ', 'å®œå¨', 'æåˆ°', 'ä¸ƒçº§']</td>\n",
       "      <td>ä»Šæ™šé¦–æˆ˜å¥–10ç‚¹å¾®ä¿¡é›†åˆï¼ä¸‹ä¸€åŸï¼ç•™å…µç•™å…ƒå®ï¼æœ‰æŠ•çŸ³æœºï¼èµ„æºæ¯å¤©30æ¬¡æå®œå¨éƒ¡æåˆ°ä¸ƒçº§ã€‚</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>['ç•™å…µ', 'ç•™ä»¤', 'èµ„æº', 'è¯¥å‡', 'å‡çº§', 'æ²¡æåŸ', 'æ', 'å®œå¨', '10', 'é¦–æˆ˜']</td>\n",
       "      <td>ç•™å…µç•™ä»¤èµ„æºè¯¥å‡çš„å‡çº§ï¼Œæ²¡æåŸçš„æå®œå¨ï¼10ç‚¹é›†åˆï¼é¦–æˆ˜ï¼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>['sg', '888']</td>\n",
       "      <td>sg888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>['æ”¶å·', 'ç§èŠ']</td>\n",
       "      <td>æ”¶å·ç§èŠ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>['w', '18370951233']</td>\n",
       "      <td>w18370951233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>['æ‰£', '1']</td>\n",
       "      <td>ä¸€ä¸ªæ˜¯ æ‰£1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                      tokens  \\\n",
       "0  ['ç¬¬', '12', 'ç« ', 'ç¬¬', '5', 'å…³', 'å‰¯æœ¬', 'n', 'èƒŒå¼º', 'é€€', 'n', 'é¬¼']                             \n",
       "1  ['åŠ ', 'å¾®ä¿¡', 'è¿›ç¾¤']                                                                           \n",
       "2  ['æŒ‚', 'å…æˆ˜']                                                                                 \n",
       "3  ['çš„è¯', 'åŠ ', 'å¾®ä¿¡', 'è¿›ç¾¤']                                                                     \n",
       "4  ['é¦–æˆ˜', 'å¥–', '10', 'å¾®ä¿¡', 'ä¸‹ä¸€åŸ', 'ç•™å…µ', 'ç•™', 'å…ƒå®', 'æŠ•çŸ³æœº', 'èµ„æº', '30', 'æ', 'å®œå¨', 'æåˆ°', 'ä¸ƒçº§']   \n",
       "5  ['ç•™å…µ', 'ç•™ä»¤', 'èµ„æº', 'è¯¥å‡', 'å‡çº§', 'æ²¡æåŸ', 'æ', 'å®œå¨', '10', 'é¦–æˆ˜']                                \n",
       "6  ['sg', '888']                                                                               \n",
       "7  ['æ”¶å·', 'ç§èŠ']                                                                                \n",
       "8  ['w', '18370951233']                                                                        \n",
       "9  ['æ‰£', '1']                                                                                  \n",
       "\n",
       "                                        content  \n",
       "0  ç¬¬12ç« ç¬¬5å…³å‰¯æœ¬æ‰“Næ¬¡èƒŒå¼ºé€€Næ¬¡ï¼Œæä»€ä¹ˆé¬¼                        \n",
       "1  åŠ æˆ‘å¾®ä¿¡æ‹‰ä½ è¿›ç¾¤                                      \n",
       "2  æŒ‚å…æˆ˜äº†                                          \n",
       "3  ä½ å»çš„è¯åŠ æˆ‘å¾®ä¿¡ï¼Œæˆ‘æ‹‰ä½ è¿›ç¾¤                                \n",
       "4  ä»Šæ™šé¦–æˆ˜å¥–10ç‚¹å¾®ä¿¡é›†åˆï¼ä¸‹ä¸€åŸï¼ç•™å…µç•™å…ƒå®ï¼æœ‰æŠ•çŸ³æœºï¼èµ„æºæ¯å¤©30æ¬¡æå®œå¨éƒ¡æåˆ°ä¸ƒçº§ã€‚  \n",
       "5  ç•™å…µç•™ä»¤èµ„æºè¯¥å‡çš„å‡çº§ï¼Œæ²¡æåŸçš„æå®œå¨ï¼10ç‚¹é›†åˆï¼é¦–æˆ˜ï¼                 \n",
       "6  sg888                                         \n",
       "7  æ”¶å·ç§èŠ                                          \n",
       "8  w18370951233                                  \n",
       "9  ä¸€ä¸ªæ˜¯ æ‰£1                                        "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ = df_final.drop(columns=['cnt'])\n",
    "df_ = df_.drop_duplicates('tokens')\n",
    "# df_.to_excel('./data/dataset_ads-20210113-1.xlsx', encoding='utf8', index=False)\n",
    "# df_.to_excel('./data/dataset_ads-20210120-1.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "# df_.to_excel('./data/dataset_ads-20210201-1.xlsx', encoding='utf8', index=False)\n",
    "df_.to_excel('./data/dataset_ads-20210201-2.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "print(df_.shape)\n",
    "df_.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
