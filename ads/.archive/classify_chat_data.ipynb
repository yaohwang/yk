{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import jieba\n",
    "import traceback\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from multiprocessing import Pool\n",
    "# from multiprocessing import get_context\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 10**5)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def apply_parallel(df, func, n=-1):\n",
    "    n = os.cpu_count() if -1 == n else n\n",
    "    \n",
    "    df_ = np.array_split(df, n)\n",
    "    with Pool(n) as p:\n",
    "    # with get_context('spawn').Pool(n) as p:\n",
    "        dfr = pd.concat(p.map(func, df_))\n",
    "        # r = p.map(func, df_)\n",
    "    p.join()\n",
    "    \n",
    "    # dfr = pd.Series(chain.from_iterable(r))\n",
    "    # print(df.shape, dfr.shape)\n",
    "    # print(dfr.head(100))\n",
    "    \n",
    "    return dfr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path, filename):\n",
    "\n",
    "    df = pd.read_csv(os.path.join(path, filename), index_col=0)\n",
    "    # print(df.shape)\n",
    "    # df.head(1)\n",
    "\n",
    "    texts = df[['content']]\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texts = texts[texts.content == 'é«˜ä»·æ”¶å´å›½é«˜è¿å·ï¼Œæœ‰æ„çš„ç§èŠï¼Œæ™šä¸Šä¹ç‚¹ç»Ÿä¸€å›žå¤ï¼']\n",
    "# texts = texts[texts.content == 'æœ‰æ„è¯·åŠ VX18526109947']\n",
    "# texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chars"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts[texts.str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## localization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter_localization(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{localization:[0-9]+\\-[0-9]+}$')].reset_index(drop=True)\n",
    "    # print(texts.shape)\n",
    "    # texts.head(1)\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## battle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_battle(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{battle:[0-9]+,ã€.*ã€‘.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## system info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_system_info(texts):\n",
    "\n",
    "    texts = texts[~texts['content'].str.contains('^{[a-z]*:[0-9]+.*}$')].reset_index(drop=True)\n",
    "    # texts.shape\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# pd.DataFrame(texts.head(10))\n",
    "texts.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lower(texts):\n",
    "\n",
    "    texts['tokens'] = texts['content'].str.strip().str.lower()\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_special_words(texts):\n",
    "\n",
    "    sw = [r'\\s+', r'{localization:[0-9]+\\-[0-9]+}', 'ä¸¶']\n",
    "\n",
    "    for _ in sw:\n",
    "        texts['tokens'] = texts['tokens'].str.replace(_, '')\n",
    "        # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## meaningless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_only(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].str.isnumeric()]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_chars(texts):\n",
    "\n",
    "    dict_merge_chars = {'è´æˆ‹' : 'è´±', 'çŠ­å¥' : 'ç‹—', 'å¥³é©¬' : 'å¦ˆ'}\n",
    "\n",
    "    for k, v in dict_merge_chars.items():\n",
    "        texts['tokens'] = texts['tokens'].str.replace(k, v)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def tokenize(df):\n",
    "    return df.apply(jieba.lcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenization(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(jieba.lcut)\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], jieba.lcut)\n",
    "    # print(len(texts))\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], tokenize)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chars + numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_numbers(i):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    m = re.match('^([a-z]+)([0-9]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    m = re.match('^([0-9]+)([a-z]+)$', i)\n",
    "    if m: return list(m.groups())\n",
    "    \n",
    "    return [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_chars_numbers(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    return list(chain.from_iterable([chars_numbers(i) for i in x])) "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_split_cn(df):\n",
    "    return df.apply(split_chars_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cn(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(split_chars_numbers)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_split_cn)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_split = {\n",
    "'å¤§é‡å‡º' : ['å¤§é‡', 'å‡º'],\n",
    "'åŠ å¾®ä¿¡' : ['åŠ ', 'å¾®ä¿¡'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_special_words(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda x: list(chain.from_iterable([dict_split.get(i,[i]) for i in x])))\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge2 = [['èµ„æº', 'å•†'], ['å¼ ', 'è¾½'], ['c', 'ä½'], ['éƒ¡', 'åŸŽ'], ['æ´—æ´—', 'ç¡'], ['å®ˆ', 'ä¸ä½'], ['åˆš', 'æœ‰äº‹'], ['å¼ƒ', 'å‘'], ['æ–°æ‰‹', 'æœ'], ['å¸¦', 'å…µ'], ['è€', 'æ ·å­'], \n",
    "               ['å†’ä¸ª', 'æ³¡'], ['ç™½', 'å«–'], ['äº²', 'å¯†åº¦'], ['æ‰“å¯‡', 'åŒª'], ['å‘¨å¹´', 'åº†'], ['å¸¦', 'èŠ‚å¥'], ['çŽ©', 'æ¸¸æˆ'], ['æ–°', 'ç‰ˆæœ¬'], ['ç§¯ç‚¹', 'å¾·'], ['ç‚¸', 'çŸ¿'], ['èµ›å­£', 'æœ'], \n",
    "               ['é‚º', 'åŸŽ'], ['æƒ³', 'åŠžæ³•'], ['ä¹Œ', 'éª“'], ['é¾Ÿ', 'å­™'], ['çº¢', 'æ‰‹æŒ‡'], ['æ­»', 'å¦ˆ'], ['2', 'é˜Ÿ'], ['ç­‰', 'ä¼š'], ['æ‰“', 'ä¸è¿‡'], ['å¥½', 'å§'], ['ç­‰', 'ä¸‹'], ['æ‰“', 'æŽ‰'], \n",
    "               ['åŽ»', 'å§'], ['ç­‰', 'ä¸€ä¸‹'], ['æ‰“', 'ä¸€ä¸‹'], ['æ‰“', 'å“ª'], ['æ”¶å…µ', 'çº¿'], ['æ’¤', 'å§'], ['ä¸', 'å®¢æ°”'], ['ç¨', 'ç­‰'], ['ä¸', 'æ‡‚'], ['æ‰“', 'ä¸äº†'], ['ä¸ä¼š', 'å§'],\n",
    "              ['ä¸', 'ç¨³å®š'], ['ä¸‹', 'ä¸€ä¸ª'], ['äºº', 'ä¸å¤Ÿ'], ['åŠªåŠ›', 'ä¸­'], ['èµŒ', 'ä¸€æŠŠ'], ['åˆ°', 'ç‚¹'], ['1', 'é˜Ÿ'], ['æ²¡çœ‹', 'æ‡‚'], ['+', '1'], ['åˆä¸ª', 'å½±'], ['ç‡•å­', 'åž'],\n",
    "              ['ä¸€', 'æ¢­å­']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_merge3 = [['é’å±±', 'ä¸æ”¹', 'ç»¿æ°´é•¿æµ'], ['æ‰“', 'ä¸', 'æ‰“'], ['æ¥', 'ä¸æ¥']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge3(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(1<i):\n",
    "        if [x[i-2],x[i-1],x[i]] in list_merge3:\n",
    "            r.insert(0, ''.join([x[i-2],x[i-1],x[i]]))\n",
    "            i -= 3\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "\n",
    "    if 1 == i:\n",
    "        r.insert(0, x[1])\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge2(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    r = []\n",
    "    i = len(x) - 1\n",
    "    \n",
    "    while(0<i):\n",
    "        if [x[i-1],x[i]] in list_merge2:\n",
    "            r.insert(0, ''.join([x[i-1],x[i]]))\n",
    "            i -= 2\n",
    "        else:\n",
    "            r.insert(0, x[i])\n",
    "            i -= 1\n",
    "            \n",
    "    if 0 == i:\n",
    "        r.insert(0, x[0])\n",
    "            \n",
    "    return r"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_3(df):\n",
    "    return df.apply(merge3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def merge_words_3(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge3)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_3)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_merge_words_2(df):\n",
    "    return df.apply(merge2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_words_2(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(merge2)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], batch_merge_words_2)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regular"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_stopwords(path):\n",
    "    stopwords = []\n",
    "    for filename in os.listdir(path):\n",
    "        with open(os.path.join(path, filename), 'r') as f:\n",
    "            stopwords.extend([w.strip() for w in f.readlines()])            \n",
    "    return stopwords"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stopwords = get_stopwords('/home/wangyh/project/document_cluster/dicts/')\n",
    "print(len(stopwords))\n",
    "stopwords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ads detect only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = ['â€¦', 'âŠ™', 'âˆ€', 'à²¡', 'Ï‰', 'à²¡', 'ðŸ˜‚', 'ðŸ˜œ', 'è°¢è°¢', 'å“ˆ', 'å“ˆå“ˆ', 'å“ˆå“ˆå“ˆ', 'å“ˆå“ˆå“ˆå“ˆ', 'æ˜Žå¤©', 'æ™šä¸Š', 'ä¸­åˆ', 'åˆ·', 'åˆ·åˆ·', 'éœ', 'å¼€', 'å¼€å¼€', 'å‡ ç‚¹', 'æ²¡', 'æ”¹', 'å†›å›¢',\n",
    "     'æ•£', 'ç»“æŸ', 'åˆ‡ç£‹', 'é˜Ÿä¼', 'æ—©', 'è¯´', 'æŒ‚å…', 'ä¸¢äºº', 'æ‰“åŒª', 'é©»é˜²', 'æ„Ÿè°¢', 'å†è§', 'æ¥åˆ·', 'æŠ±æ­‰', 'è¡Œå†›', 'é£ž', 'ç‰›', 'ç‰›ç‰›', 'åšäºº', 'å–„è‰¯', 'èµž', 'èµžèµž', 'èµžèµžèµž',\n",
    "     'å…„å¼Ÿ', 'é£Žäº‘', 'æ–‡æ˜Ž', 'é£žè¿‡æ¥', 'æ‹', 'å®Œ', 'æ‹‰', 'æ±‚è™', 'æ‰¾', 'æ¸£', 'å–Š', 'æ‹ç…§', 'è¯•è¯•', 'è°¢', 'æŠ¢', 'å‘', 'æ²¡äº‹', 'å–Š', 'å ', 'åš', 'è¯•', 'ä»Šæ™š', 'æ›´æ–°', 'ç¡è§‰', \n",
    "      'èµ°', 'å¸®å¿™', 'å¸®', 'ä¸€ä¼š', 'åƒ', 'â•¯', 'â•°', 'æºœ', 'å·', 'åˆ·ä¸åˆ·', 'â†', 'à²¥', 'åŠ é€Ÿ', 'æŽ¥', 'åš', 'è¿™æ˜¯', 'å‘±å”§', 'å…ˆæ‰“', 'åŽ¿åŸŽ', 'åŽ¿', 'æ‰“ä¸åŠ¨', 'ä¸ç”¨', 'å–é…’', 'ä¸å¥½æ„æ€',\n",
    "     'å”§å”§', 'æ²¡äº‹', 'é€Ÿåº¦', 'å…ˆ', 'æ´¾', 'å¼„', 'ç•™ä¸ª', 'çºªå¿µ', 'à¸‡', 'â€¢', 'Ì€', 'â€¢', 'Ì', 'æŠ¥å', 'æ–°', 'æ–°ç‰ˆæœ¬', 'è½®å­', 'å—²', 'å¥³é©¬', 'ä¸€æ¡', 'èˆ”', 'èŠèŠ±', 'å±è‚¡', 'ç§ƒé¡¶', 'åŽä»£', 'å…”å­', \n",
    "     'é¹°é…±', 'å…¨å®¶', 'å‘è´§', 'åˆç…§', 'æ¸¸æˆ', 'å¤ª', 'å›¢', 'é‡Œ', 'è¯…å’’', 'åˆå½±', 'ä¸€é˜Ÿ', 'äºŒé˜Ÿ', 'äºŒå›¢', 'ä¸‰å›¢', 'ä¸€ç¾¤ç¾¤', 'é˜¿çŒ«', 'éª—', 'åˆ ', 'ç»Ÿä¸€', 'ç­¾åˆ°', 'è‡ªåŠ¨', 'è€å­', 'è¯´è¯', \n",
    "      'çªå›ŠåºŸ', 'æ­»å¦ˆ', 'æ²¡å¦ˆ', 'çŸ­å‘½', 'æ¸£ç§', 'æˆå¤©', 'åªä¼š', 'äº²éº»', 'æ‰“æ³¡', 'äº²è·Œ', 'æ­»', 'å ', 'éª‚', 'èµ¢', 'æ±Ÿæ¹–', 'æ•…äºº', 'ç›¸é€¢', 'é’å±±', 'ä¸æ”¹', 'ç»¿æ°´é•¿æµ', 'ä¸‡äº‹', 'é¡ºé‚',\n",
    "     'å°”éš†', 'å’šé”µ', 'è¿›', 'å·®', 'è¿œ', 'åˆ', 'ï¼Œ', 'éƒ½', 'æ˜¯', 'å°±æ˜¯', 'çš„', 'æ›´å¥½', 'å‘å±•', 'åˆ©ç”¨', 'ï¼Œ', 'å¤§å®¶', 'ä¸€èµ·', 'ä½œæˆ˜', 'æœ‰', 'å’Œ', 'ä½ ', 'æˆ‘', 'è¿˜', 'åœ¨çº¿', 'ã€‚', '*', 'ã€', ':',\n",
    "     'æ‚¨', 'èƒ½', 'ä¸º', 'äº†', 'å°±', 'ï¼ˆ', 'ï¼Ÿ', 'å—', 'æ¥', 'äº†', 'èƒ½', 'ä¸èƒ½', 'å¤§', 'å˜›', 'è¢«', 'é‚£', 'åœ¨', 'ï¼', 'ä»–', 'å†', 'è‡ªå·±', 'åæ ‡', 'æˆªå›¾', 'ï¼', 'å‡†å¤‡', 'é€€åœº', '2é˜Ÿ', 'è¿™', \n",
    "     'ç§¯ç‚¹å¾·', 'ä¹Ÿ', 'è®©', 'ä»–ä»¬', 'å•Š', 'çŽ°åœ¨', 'ä¸ºäº†', 'ä½ å¥½', 'æ‚¨å¥½', 'æœ¬', 'æœ‰å¿—', 'é’å¹´', 'å…±åŒ', 'è¿ŽæŽ¥', 'ç›´æŽ¥', 'ç”³è¯·', 'èœ€å›½', 'åŠ æ²¹', 'ç ¸ç§', 'ç‹—', 'æ±—é—´', 'é­ä¸»', \n",
    "      'ç¥–å®—', 'çŽ°å®ž', 'åœ°ä½', 'ä¹è¶£', 'å‘¢', 'è®¤è¯†', 'æ‹¿', 'ä¸ª', 'æ¯”', 'å½“', 'ä»¬', 'ä¸€åœº', 'æŒä¹…æˆ˜', 'ç›¸ä¿¡', 'èƒœåˆ©', 'ç»ˆå°†', 'å±žäºŽ', 'æˆ‘ä»¬', 'å›¢ç»“', 'ä¸€å®š', 'èƒœåˆ©', 'æ¬¢è¿Ž', 'æ‰€æœ‰', \n",
    "      'å´å›½', 'å¤§è€', 'é»‘', 'åˆç§°', 'çœŸäºº', 'å¹¿å¤§', 'ç¾¤ä¼—', 'è¿œç¦»', 'äº²ç ', 'å—·å—·å«', 'å­é¡º', 'å„¿', 'æ‡†', 'å¾—', 'è¿™ä¸ª', 'å–”', 'çŒª', 'æ‹±', 'æµ‘èº«', 'æºƒçƒ‚', 'æ­£åœ¨', 'å–', 'è„“è¡€', 'ç–—ä¼¤',\n",
    "     'å¯æ€œ', 'å®¶é‡Œ', 'å¾—', 'å‰©', 'çŸ¥é“', 'ä¸', 'å®¹æ˜“', 'çˆ¶æ¯', 'ç­‰', 'ç€', 'æ‹¿', 'é’±', 'ä¹°', 'æ£ºæ', 'ä¸‹è‘¬', 'å‘¢', 'å¯æ˜¯', 'é‚£ç‚¹', 'çˆ¶æ¯', 'çƒ§çº¸', 'ä¸å¤Ÿ',\n",
    "      'ä¸', 'çŸ¥é“', 'å›ž', 'äº‘å¤©', 'é™ª', 'æ‰€è°“', 'â€œ', 'â€', 'è¿˜æ˜¯', 'åƒ', 'ç™žçš®ç‹—', 'ä¸€æ ·', 'ç»§ç»­', 'ç•™åœ¨', 'å¤§è…¿', 'å¯ä»¥', 'æŠ±', 'å›¢é‡Œ', 'å½“å¥´æ‰', 'æ‹­ç›®ä»¥å¾…',\n",
    "     'ç§åœ°', 'çŽ°å®ž', 'æ®‹é…·', 'å†œæ°‘', 'ä»¬', 'è¾›è‹¦', 'å•¦', 'ä¸è¾žè¾›è‹¦', 'åšæŒ', 'å¸½å­', 'å°', 'å†œæ°‘', 'æ€ç»´', 'çœŸæ˜¯', 'å•çº¯', 'å¯çˆ±', 'æƒ¹', 'äºº', 'ç–¼', 'å‘¢', 'ä¸', 'çŸ¥é“', 'ä¸', 'æ‰“', \n",
    "      'å®æœ', 'å‚»', 'ç¬”çŽ‹', 'äºº',  'å‘¢', 'æ‰“', 'å§', 'æ­¤', 'ä¸è‚–', 'å­', 'è¿˜æœ‰', 'äºº', 'é›†ç»“', 'ç•™å¿µ', 'æœ€åŽ', 'ä¸€æœŸ', 'å¥½', 'å‡‘ä¸ª', 'è„¸ç†Ÿ', 'ä»€ä¹ˆ', 'æ„æ€', 'å—¯', 'å¥½', 'æƒ…å†µ', 'äºŒè´§',\n",
    "     'ä»Šå¤©', 'æ•Œäºº', 'æœ‹å‹', 'è¿™é‡Œ', 'æ‰“', 'å¯èƒ½', 'å»¶è¿Ÿ', 'å‡ åˆ†é’Ÿ', 'å°½é‡', 'ä¿è¯', 'æ¯', 'ä¸€ä½', 'è¿™ä¸ª', 'æ—¶å€™', 'å‘µå‘µ', 'å‘µ', 'å¥½', 'å˜ž', 'æ©æ€¨', 'ä¸€ç¬‘', 'æ³¯', 'æ©ä»‡',\n",
    "     'å•¥', 'æƒ…å†µ', 'ä½ ä»¬', 'å“¦', 'å¥½', 'é¡¶æˆ˜', 'æ¸…äºº', '(', ')', 'åˆš', 'ä¸Šçº¿', 'æˆ‘ä»¬', 'åŠ æ²¹', 'æˆå‘˜', 'å…¨ä½“', 'é›†åˆ', 'æ°‘å¿ƒ', 'ä»»åŠ¡', 'å§', 'å“ª', 'è®¤è¯†', 'æ¸…æ¥š', 'ä¹ˆ', 'åˆ†é’Ÿ', 'æ‰“ä¸è¿‡', \n",
    "     'åˆ«', 'å‘€', 'æˆ˜åŠ›', 'ä»¥åŽ', 'å†å‡', 'ç•™ç‚¹', 'å…µ', 'åŽ»', 'æœ‰äºº', 'åœŸåŒª', 'æ¥ä¸ª', 'å¸®å¸®å¿™', 'ï¼š', 'ï¼‰', 'æœ‰äºº', 'ç¢°ç“·', 'åŽ‰å®³', 'å‘€', 'æ€Žä¹ˆ', 'ä¸ç†', 'å› ä¸º', 'è§‰å¾—', 'å’‹æ ·',\n",
    "      'æ…¢æ…¢', 'çŽ©å§', 'äº', 'å¤§å‘', 'åˆ«', 'äº’ç›¸', 'ä¼¤å®³', 'å‘Šè¯‰', 'æ‰“é‡Ž', 'æœ‰ç‚¹', 'è¿‡åˆ†', 'å›žå®¶', 'å…¬å‘Š', 'çœ‹', 'åŽ»', 'å°†', 'æˆ‘å¼€', 'ç›¸å¿˜', 'äºŽ', 'åˆ°', 'æ—è¾¹', 'ä¹', 'æœ‰ç¼˜åƒé‡Œ', 'ç›¸ä¼š',\n",
    "     'æœ‰ç¼˜', 'å„ä½', 'çœŸ', 'æ— èŠ', 'åˆ†é’Ÿ', 'æˆ‘æ¥', 'çœ‹çœ‹', 'æ', 'æ²¡æœ‰', 'ä¸€ä¸‹', 'ä¸€ä¸ª', 'ä¸Š', 'ä¸‹', 'ç”¨', 'ä¸è¦', 'ä½ç½®', 'è¿™ä¹ˆ', 'ç¹è£', 'è°', 'çŽ©', 'ç§’', 'æŠŠ', 'å¤šå°‘', 'å¤š', 'é­å›½',\n",
    "      'ä¸€äºŒä¸‰å››äº”', 'è¿‡æ¥', 'ä¹ç‚¹', 'é•¿å®‰', 'æ¬¡', 'ä¸æ˜¯', 'å¼€å§‹', 'é˜µå®¹', 'ä»¥ä¸Š', 'æ—¶é—´', 'å¾ˆ', 'æ¸…', 'å¸¦', 'åŸŽ', 'æ‰“é£ž', 'ä¼š', 'å’¯', 'åœ°ç‚¹', 'éƒ¡åŸŽ', 'ç•™å½±', 'æ´›é˜³', 'æƒ³', 'æœ‰æ²¡æœ‰', \n",
    "      'æž', 'ç¥', 'æå‰', 'éšä¾¿', 'çœ‹è§', 'ä¸ªäºº', 'èµ·æ¥', 'ä¸€ç‚®', 'ä¸è¡Œ', 'å«', 'å¯¹', 'è½ä½', 'ä¸Šè½¦', 'å¤©å¤©', 'ä¸å¥½', 'ä»¤', 'åˆ†', 'åŒº', 'é«˜', 'çŽ‹', 'å…¶ä»–', 'é€šå‘Š', 'å›´åŸŽ', 'è”ç›Ÿ',\n",
    "     'å¿«', 'å…µçº¿', 'åœ°æ–¹', 'å¤Ÿ', 'å’‹', 'çº¿', 'å°æ—¶', 'è·‘', 'åŽé¢', 'æ— ', 'èµ·', 'é‚£ä¸ª', 'èœ€', 'æ•£äºº', 'éƒ¡', 'æ‰€æœ‰äºº', 'æœ¬äºº', 'åŽŸå§‹', 'å¨±ä¹', 'èµ¶ç´§', 'ä¸€ç‚¹', 'cä½', 'é”™', 'å»ºç­‘',\n",
    "     'æˆéƒ½', 'ä¸€çŽ¯', 'å‡‘', 'å¸Œæœ›', 'æ’¤', 'é£žè¿‡åŽ»', 'é…åˆ', 'å…¶ä»–äºº', 'è¿‡', 'å‡ºå…µ', 'é‚£ä¹ˆ', 'éšæœº', 'æœ€å¥½', 'å…¨éƒ¨', 'æ‰', 'ä¸€ç›´', 'é¢†', 'ä¸­', ',', 'å¯', 'çŽ°', 'å¼€å¿ƒ', 'è·Ÿ', 'ä¸»å…¬',\n",
    "     'ç‚¸', 'é©¬ä¸Š', 'æœ¬æœ', 'ç…§', 'è¿‡èŠ‚', 'å¤§å°', 'æŠ¥ä»‡', 'ä¸‰å›½', 'ä»ŠåŽ', 'ä»¥å‰', 'æ¶ˆ', 'å¼Ÿå…„ä»¬', 'æ—©åˆ°', 'å·²ç»', 'è‡ªç«‹', 'åŸŽæ± ', 'å‡†æ—¶', 'ç§ç”°', 'æ˜¥èŠ‚', 'å·²æ”¾', 'ä¸‹ä½', 'å¤„',\n",
    "     'æ–¹å‘', 'å‚åŠ ', 'è¯¥', 'æµªè´¹', 'ç…§ç‰‡', 'ä¸–ç•Œ', 'è§', 'ä¸€æ¬¡', 'é›†ä½“', 'åšä¸ª', 'åˆ°ä½', 'å·®è·', 'å¤©', 'å°±ä½', 'æ‰“åŸŽ', 'æ²¡å‡ºæ¯', 'å¥‡æ€ª', 'å¥½åƒ', 'åŸå¤´', 'ä¸äº†', 'å“ªä¸ª', 'ä¸åˆ°', \n",
    "     'æ²¡å…µ', 'å½»åº•', 'æŠ¥', 'è®¸æ˜Œ', 'å“¥å“¥', 'åˆšåˆš', 'å†æ¥', 'åˆšæ‰', 'å›žæ¥', 'å¦‚æžœ', 'å‡ºæ¥', 'å‡ æ¬¡', 'æ€»æ˜¯', 'åº”è¯¥', 'å¥½å¤š', 'è¿‘', 'é•¿æ²™', 'ä¸€å…µ', 'ä¸æ¥', 'å¤ªè™š', 'éšæ„', 'åªæœ‰',\n",
    "     'ç­‰ä¼š', 'æ¡æ¸£ç§', 'æ­¤æ—¶æ­¤åˆ»', 'è¢«æŽ§', 'ç¬¬ä¸€', 'æ·±æ›´åŠå¤œ', 'å¹³æ°‘', 'éœ¸æœ', 'è„¸', 'æœ¬äº‹', 'æ‰“å‡º', 'åˆ«è¯´', 'æŽ§å·', 'æœ‰é’±', 'åˆ«äºº', 'è¢«æŽ§', 'æ®‹', 'ç‹—ä¸œè¥¿', \n",
    "      'åœ°å„¿', 'éš†å†¬', 'å¼º', 'å’šå’š', 'é”µ', 'æ•´ä¸ª', 'æŒ¨ä¸ª', 'ç¾¤', 'bb', 'ç¬”', 'å†…', 'é—´', 'å†…', 'ç•œç”Ÿ', 'å­—', 'å°å', 'ç«¥å­', 'å–œ', 'ç”·äºº', 'ç”·é£Ž', 'è€…', 'ä¸æ€•', 'py',\n",
    "     'ç‹—å¥´æ‰', 'å˜´ä¸Š', 'ç«‹äºº', 'è®¾æ—¶', 'è¿ž', 'å½“çœŸ', 'ä½†', 'ç»ä¸èµ·', 'ä»»ä½•', 'è€ƒéªŒ', 'å•ªå•ª', 'æ‰“è„¸', 'ä¸¤é¢ä¸‰åˆ€', 'å°äºº', 'æ™š', 'ç‚¹', 'è‡ªç”±', 'å¼€æ”¾', '1é˜Ÿ', 'å¹³å®‰', 'å–œä¹', 'æœˆ',\n",
    "     'è§£æ•£', 'éº»çƒ¦', 'éšè¿', 'ä¸œç€›', 'é¬¼å­', 'é‡Žç§', 'å…¨é£ž', 'ä¿æŒ', 'é˜Ÿå½¢', 'å¯’æ±Ÿ', 'å­¤å½±', 'ä½•å¿…', 'æ›¾', 'ç›¸è¯†', 'ç¡', 'æ™šå®‰', 'æ‰‹æœº', 'æ²¡ç”µ', '123', 'èŒ„å­', 'æ”¶å…µçº¿', 'å‡', \n",
    "      'è¿›æ­¥', 'å¾ˆå¤§', 'æ»¡çº§', 'é‡', 'å†ä¹Ÿ', 'ä¸æƒ³', 'æ¬ºè´Ÿ', 'å¼±å›¢', 'æ¬ºè´Ÿ', 'æœ¬å›½', 'å¼±å°', 'åˆ†æ‰‹', 'æ‰“æŽ‰', 'è¡Œ', 'æ•£ä¼™', 'è‡­è¡¨å­', 'ä¸€ç»„', 'è¿‡åŽ»', 'æŽ‰', 'å…¨ä¸€çŽ¯', 'æ ¼å­',\n",
    "     'æ ¡åœº', 'å‰¯', 'å†ä¹Ÿ', 'ä¸è§', 'å“', 'è·³', 'å‚»ç¬”', 'æ¯å¤©', 'æ—©ç‚¹', 'ä¼‘æ¯', 'ç§°å·', 'åºŸ', 'å…«å§“', 'å®¶å¥´', 'å‡ æŠŠ', 'æ¶æ˜¯', 'å‡ æŠŠ',\n",
    "     'èµ›å­£', 'æœå‰', 'ä¸€å¤©', 'ä¸€è·¯', 'æ¶ªé™µ', 'æœ‰ç©º', 'æŒ‰', 'é¡ºåº', 'æ•´ä½“', 'æŽ’åˆ—', 'ç‚¹ä¼š', 'å®‰æŽ’', 'å¤§æ®¿', 'ä¸å¸¦', 'åé˜²', 'å¥½å‹', 'å¾’å¼Ÿ', 'å°è½¦', 'æ­»å…‰å…‰',\n",
    "     'æ˜Žæ™š', 'é•¿å®‰åŸŽ', 'æœ¬åŒº', 'æä¾›', 'å…·ä½“åœ°å€', 'æ—¶å‘', 'åŸŽåˆ°', 'é“è¡€', 'å…·ä½“', 'åŠæ‰“', 'å½•å±', 'æ³¨æ„', 'æœ‰åˆ·', 'æš‚æ—¶', 'ç¦»å›¢', 'å—é—¨', 'å†›æ——', 'è´±ç‹—', 'çˆ½', 'ç‹—å¨˜å…»',\n",
    "    'è¥¿å—', 'é—¨', 'å†›æ——', 'å›´', 'ä¸‰çŽ¯', 'æŠ“ç´§', 'éª‘å…µ', 'è¿›æ”»',  'æŠ¢æ–°', 'ç‰ˆæœ¬', 'é¦–ç«™', 'å¤©çŽ‹', 'ç›–åœ°', 'è™Ž', 'å¤§ç™½', 'å–', 'è“è‰²', 'èœ€æ±‰', 'æ——', 'å¤§ç™½', 'å‘Š', 'å¦»',\n",
    "     'é“¶æ²³', 'å¥¥ç‰¹æ›¼', 'æ²™æ¯”', 'æ€»å–Š', 'çŽ›', 'æ²¡é’±åˆ«', 'æ¯”æ¯”', 'èƒ½è€', 'çº³', 'é—·', 'ä¸å¤š', 'é±¼å', 'å„è·¯', 'è‹±é›„', 'å›è´¼', 'æ‰“å®¶åŠ«èˆ', 'åŽŸ', 'åŒºä¸åˆ†', 'å›½ç±', 'æ‹ä¸ª',\n",
    "     'é’±å¤§ç‹—', 'æœŸå¾…', 'ä¸Ž', 'å†æ¬¡', 'ç›¸é‡', 'æœŸ', 'æˆªæ­¢', 'æƒ³æ¥', 'é£žè¡Œ', 'çŽ©è€', 'ä¸‡ä¸€', 'ä½Žæˆ˜å›¾']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = list(set(sw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.extend(sw)\n",
    "len(stopwords)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def filter_sw(df):\n",
    "    \n",
    "    # return pd.Series([w for w in x if w not in stopwords])\n",
    "    \n",
    "    # for w in x:\n",
    "       # print(w, w not in stopwords)\n",
    "    # return [w for w in x if w not in stopwords]\n",
    "    \n",
    "    return df.apply(lambda l: [w for w in l if w not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stopwords(texts):\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(lambda l: [w for w in l if w not in stopwords])\n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], lambda l: [w for w in l if w not in stopwords])\n",
    "    \n",
    "    # texts['tokens'] = apply_parallel(texts['tokens'], filter_sw)\n",
    "    # texts = texts.dropna()\n",
    "    \n",
    "    \n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeats(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    # print(x)\n",
    "    \n",
    "    if 2 > len(x): return x\n",
    "    \n",
    "    if 2 == len(x): \n",
    "        return [x[0]] if x[0] == x[1] else x\n",
    "    \n",
    "    r = [x[i] for i in range(len(x)-2) if x[i] != x[i+1]]\n",
    "    if x[-1] == x[-2]:\n",
    "        r.append(x[-2])\n",
    "    else:\n",
    "        r.extend(x[-2:])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_repeats(texts):\n",
    "\n",
    "    # è°¢è°¢, è°¢è°¢\n",
    "    # å–é…’, å–é…’\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].apply(remove_repeats)\n",
    "    # print(len(texts))\n",
    "    # texts.head(1)\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'v' + num\n",
    "\n",
    "def is_v_num(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and 'v' == x[0] and x[1].isnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_v_num(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_v_num)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num + 'çº§'\n",
    "\n",
    "def is_num_ji(x):\n",
    "    \"\"\" tool\n",
    "    \"\"\"\n",
    "    return 2 == len(x) and x[0].isnumeric() and 'çº§' == x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_num_ji(texts):\n",
    "\n",
    "    texts = texts[~texts['tokens'].apply(is_num_ji)]\n",
    "    # print(len(texts))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_length(texts):\n",
    "    # texts['length'] = texts['tokens'].apply(len)\n",
    "    \n",
    "    texts = texts[texts['tokens'].apply(len) > 1]\n",
    "    # print(len(texts))\n",
    "    # print(texts.tail(10))\n",
    "\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# df = pd.DataFrame(texts)\n",
    "# df.head(1)\n",
    "texts.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(texts):\n",
    "\n",
    "    # df = df.astype(str).value_counts().reset_index().rename(columns={0:'cnt'})\n",
    "    # df.head(1)\n",
    "\n",
    "    texts['tokens'] = texts['tokens'].astype(str)\n",
    "    df = texts.groupby('tokens').agg({'tokens':['count'], 'content':[lambda x: pd.DataFrame.head(x,1)]}).reset_index()\n",
    "\n",
    "    df.columns = ['tokens', 'cnt', 'content']\n",
    "    df = df[['tokens','content', 'cnt']]\n",
    "    df = df.sort_values('cnt', ascending=False).reset_index(drop=True)\n",
    "    # df.head(1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## freq more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_more(df, n=None):\n",
    "\n",
    "    dfn = df[df['cnt'] >= n]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq_between(df, n1=None, n2=None):\n",
    "\n",
    "    dfn = df[(df['cnt'] >= n1) & (df['cnt'] < n2)]\n",
    "    # print(dfn.shape)\n",
    "\n",
    "    return dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df5 = df[df['cnt'] >= 5]\n",
    "print(df5.shape)\n",
    "df5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df4 = df[(df['cnt'] < 5) & (df['cnt'] >= 4)]\n",
    "print(df4.shape)\n",
    "df4"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df3 = df[(df['cnt'] < 4) & (df['cnt'] >= 3)]\n",
    "print(df3.shape)\n",
    "df3"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = defaultdict(int)\n",
    "\n",
    "for tokens in df3.tokens:\n",
    "    for t in set(eval(tokens)):\n",
    "        counter[t] += 1\n",
    "        \n",
    "counter = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "counter"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df2 = df[(df['cnt'] < 3) & (df['cnt'] >= 2)]\n",
    "print(df2.shape)\n",
    "df2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dfn = df[df['cnt'] > 1]\n",
    "print(dfn.shape)\n",
    "# dfn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df1 = df[df['cnt'] <= 1]\n",
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2æœŸ æ ‡æ³¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracts(path, keep, filename):\n",
    "\n",
    "    try:\n",
    "        \n",
    "        texts = load(path, filename)\n",
    "        # texts = texts.head(10)\n",
    "\n",
    "        pipeline = [\n",
    "            filter_localization, \n",
    "            filter_battle,\n",
    "            filter_system_info,\n",
    "\n",
    "            clean_lower,\n",
    "            clean_special_words,\n",
    "            clean_numeric_only,\n",
    "            merge_chars,\n",
    "\n",
    "            tokenization,\n",
    "            split_cn,\n",
    "            split_special_words,\n",
    "            merge_words_3,\n",
    "            merge_words_2,\n",
    "            filter_stopwords,\n",
    "            filter_repeats,\n",
    "\n",
    "            filter_v_num,\n",
    "            filter_num_ji,\n",
    "            filter_length,\n",
    "                   ]\n",
    "\n",
    "        for f in pipeline:\n",
    "            # print(f)\n",
    "            t0 = time.time()\n",
    "            texts = f(texts)\n",
    "            # print(time.time()-t0)\n",
    "\n",
    "        df = extract(texts)\n",
    "        # print(df.shape)\n",
    "\n",
    "        # dfn = freq_more(df, 5)\n",
    "        # dfn = freq_between(df, 3, 5)\n",
    "        # dfn = freq_more(df, 2)\n",
    "        # dfn = freq_between(df, 1, 2)\n",
    "        dfn = keep(df)\n",
    "        print(dfn.shape)\n",
    "        # print()\n",
    "\n",
    "        return dfn\n",
    "    \n",
    "    except:\n",
    "        print(filename)\n",
    "        traceback.print_exc()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/home/wangyh/data/yk-sgz2017-chat/data-2-20201223'\n",
    "# filename = '2020_09_25.csv'\n",
    "# filename = '2020_09_24.csv'\n",
    "# filename = '2020_08_20.csv'\n",
    "\n",
    "path = '/home/wangyh/data/yk-sgz2017-chat/data-7-20210120'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 325M\r\n",
      "drwxrwxr-x  2 wangyh wangyh  4.0K Jan 20 11:29 .\r\n",
      "drwxrwxr-x 10 wangyh wangyh   233 Mar 29 16:46 ..\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_10_01.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_10_02.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.4M Jan 20 11:29 2020_10_03.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_10_04.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_10_05.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2020_10_06.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.4M Jan 20 11:29 2020_10_07.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.3M Jan 20 11:29 2020_10_08.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2020_10_09.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_10_10.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2020_10_11.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.4M Jan 20 11:29 2020_10_12.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.0M Jan 20 11:29 2020_10_13.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  5.3M Jan 20 11:29 2020_10_14.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.8M Jan 20 11:29 2020_10_15.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.9M Jan 20 11:29 2020_10_16.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.7M Jan 20 11:29 2020_10_17.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_10_18.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_10_19.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_10_20.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  5.4M Jan 20 11:29 2020_10_21.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.9M Jan 20 11:29 2020_10_22.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.4M Jan 20 11:29 2020_10_23.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh 1004K Jan 20 11:29 2020_10_24.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.4M Jan 20 11:29 2020_10_25.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.3M Jan 20 11:29 2020_10_26.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.5M Jan 20 11:29 2020_10_27.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.5M Jan 20 11:29 2020_10_28.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.9M Jan 20 11:29 2020_10_29.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.8M Jan 20 11:29 2020_10_30.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.9M Jan 20 11:29 2020_10_31.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.8M Jan 20 11:29 2020_11_01.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.6M Jan 20 11:29 2020_11_02.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.8M Jan 20 11:29 2020_11_03.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_11_04.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_11_05.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_11_06.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_11_07.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.3M Jan 20 11:29 2020_11_08.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_09.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_11_10.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2020_11_11.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_11_12.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_13.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_14.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_15.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_16.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_11_17.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2020_11_18.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.9M Jan 20 11:29 2020_11_19.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_11_20.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_11_21.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.9M Jan 20 11:29 2020_11_22.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2020_11_23.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_24.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2020_11_25.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_11_26.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.0M Jan 20 11:29 2020_11_27.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.3M Jan 20 11:29 2020_11_28.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.5M Jan 20 11:29 2020_11_29.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.3M Jan 20 11:29 2020_11_30.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_12_01.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_12_02.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_12_03.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_12_04.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_12_05.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.9M Jan 20 11:29 2020_12_06.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_12_07.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2020_12_08.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_12_09.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_12_10.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2020_12_11.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_12_12.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_12_13.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2020_12_14.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.5M Jan 20 11:29 2020_12_15.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.9M Jan 20 11:29 2020_12_16.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.1M Jan 20 11:29 2020_12_17.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2020_12_18.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2020_12_19.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.0M Jan 20 11:29 2020_12_20.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.5M Jan 20 11:29 2020_12_21.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.7M Jan 20 11:29 2020_12_22.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.1M Jan 20 11:29 2020_12_23.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.5M Jan 20 11:29 2020_12_24.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_12_25.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.6M Jan 20 11:29 2020_12_26.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.7M Jan 20 11:29 2020_12_27.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.5M Jan 20 11:29 2020_12_28.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.3M Jan 20 11:29 2020_12_29.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.0M Jan 20 11:29 2020_12_30.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.8M Jan 20 11:29 2020_12_31.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  2.6M Jan 20 11:29 2021_01_01.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.2M Jan 20 11:29 2021_01_02.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2021_01_03.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.1M Jan 20 11:29 2021_01_04.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.9M Jan 20 11:29 2021_01_05.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.8M Jan 20 11:29 2021_01_06.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.2M Jan 20 11:29 2021_01_07.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2021_01_08.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.2M Jan 20 11:29 2021_01_09.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.6M Jan 20 11:29 2021_01_10.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.8M Jan 20 11:29 2021_01_11.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  3.5M Jan 20 11:29 2021_01_12.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  5.1M Jan 20 11:29 2021_01_13.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  4.0M Jan 20 11:29 2021_01_14.csv\r\n",
      "-rw-rw-r--  1 wangyh wangyh  1.6M Jan 20 11:29 2021_01_15.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -alh /home/wangyh/data/yk-sgz2017-chat/data-7-20210120"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "%time extracts(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_similarity(s1, s2):\n",
    "    _s1 = set(s1)\n",
    "    _s2 = set(s2)\n",
    "    return len(_s1 & _s2) / max(len(_s1), len(_s2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_similarity(s1, s):\n",
    "    for _ in s1:\n",
    "        # if abs(len(_)-len(c)) < 3 and edit_distance.SequenceMatcher(_, c).ratio() > 0.9:\n",
    "        if abs(len(_)-len(s)) < 3 and set_similarity(_,s) > 0.9:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(keep, num=None):\n",
    "\n",
    "    # split\n",
    "    # print(len(os.listdir(path)))\n",
    "    t0 = time.time()\n",
    "    with Pool(8) as p:\n",
    "        tasks = os.listdir(path)\n",
    "        \n",
    "        # dfs = p.map(partial(extracts, path, keep), tasks)\n",
    "        \n",
    "        dfs = []\n",
    "        for _ in tqdm(p.imap(\n",
    "            partial(extracts, path, keep), \n",
    "            tasks\n",
    "        ), total=len(tasks)):\n",
    "            dfs.append(_)\n",
    "        \n",
    "    p.join()\n",
    "    print(time.time() - t0)\n",
    "\n",
    "    # count\n",
    "    df_final = pd.concat(dfs).reset_index(drop=True)\n",
    "    print(df_final.shape)\n",
    "\n",
    "    df_ = df_final.drop(columns=['cnt'])\n",
    "    df_ = df_.drop_duplicates('tokens')\n",
    "\n",
    "    print(df_.shape)\n",
    "\n",
    "    if num is not None: df_ = df_.head(num)\n",
    "\n",
    "    # drop duplicates\n",
    "        \n",
    "    r = defaultdict(list)\n",
    "    r[0] = []\n",
    "\n",
    "    for c in tqdm(df_.content.to_list()):\n",
    "        flag = True # not exist\n",
    "\n",
    "        for i in range(max(0,len(c)-3), min(max(r.keys()),len(c)+3)):\n",
    "            if in_similarity(r[i], c):\n",
    "                flag = False\n",
    "                break\n",
    "                \n",
    "        if flag:\n",
    "            r[len(c)].append(c)\n",
    "\n",
    "    r = list(chain.from_iterable(r.values()))\n",
    "    print(len(r))\n",
    "\n",
    "    return pd.DataFrame({'text': r})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = get(partial(freq_more, n=2))\n",
    "print(df1.shape)\n",
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = get(partial(freq_between, n1=1, n2=2), num=int(4e4))\n",
    "print(df2.shape)\n",
    "df2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[df2.text.apply(lambda x: not in_similarity(df1.text.to_list(), x))]\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['version'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('./data/dataset_ads-20210420-2.csv', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip -q install edit_distance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import edit_distance\n",
    "\n",
    "ref = [1, 2, 3, 4]\n",
    "hyp = [1, 2, 4, 5, 6]\n",
    "\n",
    "sm = edit_distance.SequenceMatcher(a=ref, b=hyp)\n",
    "print(sm.get_opcodes())\n",
    "print(sm.ratio())\n",
    "print(sm.get_matching_blocks())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set('123') & set('423')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "set_similarity('123', '234')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# df_.to_excel('./data/dataset_ads-20210113-1.xlsx', encoding='utf8', index=False)\n",
    "# df_.to_excel('./data/dataset_ads-20210120-1.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "# df_.to_excel('./data/dataset_ads-20210201-1.xlsx', encoding='utf8', index=False)\n",
    "# df_.to_excel('./data/dataset_ads-20210201-2.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "# df_.to_excel('./data/dataset_ads-20210420-1.xlsx', encoding='utf8', index=False)\n",
    "df_.to_excel('./data/dataset_ads-20210420-2.xlsx', encoding='utf8', index=False)\n",
    "\n",
    "print(df_.shape)\n",
    "df_.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
